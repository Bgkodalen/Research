\chapter{Polynomial ring of Association schemes}\label{schoenberg}
Chapter $\ref{psdcone}$ examined a particular set of matrices in the Bose-Mesner algebra of an association scheme which gave us Gram matrices of finite sets $X\subset \mathbb{S}^{m-1}$ for some fixed positive integer $m$. In particular, we found that the existence of any association scheme implies the existence of sets of vectors with very few possible inner products. While we may use this correlation to create new spherical $t$-distance sets anytime we have an association scheme, we may also use this implication to rule out possible parameter sets based on the properties of the resultant vector sets. For instance, Delsarte et al.\ in \cite{Delsarte1975} found bounds on the size of any $t$-distance set based solely on the dimension of the ambient space and the inner products allowed between vectors. Later, in \cite{Delsarte1977}, they were also able to prove a $t$-distance set is a spherical design if and only if the first $s$ Gegenbauer polynomials summed over the inner products give zero as a result. This characterization of spherical designs allowed Suda \cite{Suda2011} to characterize when a the first idempotent in a $Q$-polynomial ordering would result in a spherical design. Many of these results are based around a family of single-variable polynomials known as the Gegenbauer polynomials, and a result of Sch\"{o}nberg \cite{Schoenberg1942} which implies that each Gegenbauer polynomial will result in a positive semi-definite matrix when applied to a Gram matrix elementwise.\par
In this chapter, we will define the Gegenbauer polynomials and review relevant definitions and theorems. We will then use a consequence of Sch\"{o}nberg's theorem to provide a general constraint on the minimal idempotents of association schemes. We then leverage properties of association schemes to derive parameter restrictions on a general association scheme, noting where these restrictions become non-trivial. We then examine these restrictions more closely in the setting of $Q$-polynomial association schemes where we give three non-trivial constraints. Finally, we will give examples of new parameter sets which are ruled out by these constraints and consider the case of $4$-class $Q$-bipartite association schemes where one constraint in particular greatly restricts the feasible parameter sets. Below is a list of main theorems which will appear in this chapter.
\begin{restatable*}{thm}{schAS}\label{schoen-as}
	Let $(X,\mathcal{R})$ be an association scheme with minimal idempotents $E_0,\dots,E_d$ and Krein matrices $L_0^*,\dots,L_d^*$. Let $0\leq i\leq d$ be given and define $m_i:=\text{rank}\left(E_i\right)$. Then for any choice of $k>0$, there exist non-negative constants $\theta_{kj}$ for $0\leq j\leq d$ so that
	\begin{equation}\label{ELGeg}
	Q_k^{m_i}\circ\left(\frac{\vert X\vert}{m_i}E_i\right) = \sum_j \theta_{kj} E_j;\qquad	Q_k^{m_i}\left(\frac{1}{m_i}L_i^*\right) = \frac{1}{\vert X\vert}\sum_j \theta_{kj} L_j^*\\
	\end{equation}
	In each case, $\theta_{kj}$ is the eigenvalue of the generated matrix corresponding to the eigenspace $V_j$. Further, $\theta_{kj}$ is nonzero only if $E_j$ is contained in the Schur subalgebra generated by $E_i$.
\end{restatable*}
\begin{restatable*}{thm}{degbnd}
	\label{degreebound}
	Let $(X,\mathcal{R})$ be an association scheme with first and second eigenmatrices $P$ and $Q$. Let $0\leq i\leq d$ be given and define $\lambda_j:=\frac{Q_{j,i}}{m_j}$, $v_j = P_{0j}$. Define $k^*$ to be the smallest integer such that
	\[\sum_{j=1}^d \vert\lambda_j\vert^{2k^*}v_j(1+\lambda_j^2)\leq\frac{1}{\vert X\vert}.\]
	Then the conditions described in Theorem \ref{ELGeg} are vacuous for degrees $k\geq k^*$.
\end{restatable*}
\begin{restatable*}{thm}{antbnd}
	Let $(X,\mathcal{R})$ be a 3-class $Q$-antipodal association scheme. Then every bound arising from using $\frac{\vert X\vert}{m_1}E_1$ as the Gram matrix in \ref{schoen-as} is vacuous.
\end{restatable*}

\section{Gegenbauer Polynomials}
In this section we will use polynomial rings and quotient rings, thus we begin this section with a brief review of the topic, referring to \cite{Gallian}(TODO: Bad reference here) for a more complete introduction. In all that follows let $m$ be a fixed positive integer and define $\scR:=\bbR[x_1,\dots,x_m]$. A \emph{monomial} is defined as a (possibly empty) product of the variables $x_1,\dots,x_m$ and given a monomial $t =\prod_{i=1}^{m}x_i^{d_i}$ ($d_i\in \bbZ^+$), the \emph{degree} of $t$ is defined as $\text{deg}(t) = \sum_i d_i$. A polynomial $f\in\scR$ may be represented uniquely as a (finite) linear combination of distinct monomials $f =\sum_i\alpha_it_i$ and $\text{deg}(f) = \max\left\{\text{deg}(t_i)\right\}$. For each variable $x_j$, we define the \emph{derivative with respect to $x_j$} of a monomial as $\frac{\partial}{\partial x_j}t = d_jx_j^{d_j-1}\prod_{i\neq j}x_i^{d_i}$ and extend the definition linearly for any polynomial in $\scR$. For $f\in \scR$, $f$ is \emph{homogeneous} if there exists some constant $d\in\bbZ^+$ such that $\text{deg}(t)=d$ for every monomial $t$ in $f$. Further, $f$ is \emph{harmonic} if $\Delta f = \sum_i \left(\frac{\partial}{\partial x_i}\circ\frac{\partial}{\partial x_i}\right)\left(f\right) = 0$.\par
Let $f\in\scR$ be given and define the ideal of $f$ as $(f) = \left\{gf:g\in\scR\right\}$. We then define equivalence classes $[g]_f = \left\{h\in\scR: g-h\in(f)\right\}$. The quotient ring $\nicefrac{\scR}{(f)}$ is given as the set of equivalence classes $\left\{[g]_f:g\in\scR\right\}$. We will often suppress the subscript if it is clear from the context. Let $S^{m-1}\subset \bbR^m$ be the $m-1$ dimensional sphere, we define the set of polynomials on the sphere as
\[\text{Pol}(S^{m-1}):= \nicefrac{\scR}{\left(1-\sum_ix_i^2\right)}.\]
We say a polynomial $f\in\scR$ is \emph{harmonic on the sphere} if there exists a harmonic polynomial $g\in[f]$. Similarly, a polynomial is \emph{homogeneous on the sphere} if there exists a homogeneous polynomial $g\in[f]$. Finally, we say a polynomial $f\in\bbR$ is \emph{zonal} if there exists a vector $a\in \bbR^m$ and a single-variable polynomial $g(t)\in\bbR[t]$ such that $f(x) = g(\left<a,x\right>)$ for all $x\in S^{m-1}$. Note that since $h\in \left(1-\sum_ix_i^2\right)$ implies $h(x) = 0$ for all $x\in S^{m-1}$, this requirement is independent of the representative chosen from the equivalence class.\par
We now introduce a particular set of polynomials arising from the context of spherical harmonics polynomials. The Gegenbauer polynomials in dimension $m$ are defined using the three-term recurrence:
\begin{equation}\label{recurrence}
Q_k^m(t) = \frac{(2k+m-4)tQ_{k-1}(t) - (k-1)Q_{k-2}(t)}{k+m-3} \qquad k\geq 2,
\end{equation}
\begin{equation*}
Q^m_0(t) = 1\qquad Q^m_1(t) = t.
\end{equation*}
We use these initial polynomials so that $Q^m_k(1) = 1$ for all $k\geq 0$. We will suppress the superscript $m$ anytime this is clear in the context. The first six Gegenbauer polynomials are as follows,
\[\begin{aligned}
&Q_0(t)=1,\qquad
Q_1(t)=t,\qquad
Q_2(t)=\frac{mt^2 - 1}{m-1},\qquad
Q_3(t)=\frac{(m+2)t^3 - 3t}{m-1},\qquad\\
Q_4(t)=&\frac{(m+4)(m+2)t^4 - 6(m+2)t^2+3}{m^2-1},\qquad
Q_5(t)=\frac{(m+6)(m+4)t^5-10(m+4)t^3+15t}{m^2-1}\\
\end{aligned}\]\newpage
\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=.4]{gegenbauer_polynomials.PNG}
		\caption[Gegenbauer polynomials]{Gegenbauer polynomials with degree 1 through degree 5 with $m=10$.}\label{gegpic}
	\end{center}
\end{figure}
\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=.4]{roots.PNG}
		\caption[Roots of Gegenbauer polynomials]{Roots of the five Gegenbauer polynomials plotted in Figure \ref{gegpic}.}\label{rootpic}
	\end{center}
\end{figure}
\begin{thm}\label{gegprop}
	For each $m,i\in\bbZ^+$ and $a\in\bbR^m$, the Gegenbauer polynomial $Q_i^m\left(\left<a,x\right>\right)$ is zonal and both homogeneous and harmonic on the sphere.
\end{thm}
\begin{proof}
	The zonal condition is satisfied trivially. The other two conditions follow from $F^m_k(x)\in\left[Q_i^m(\left<a,x\right>)\right]$ for 
	\[F_k^m(x)= \left<x,x\right>^{\lfloor\frac{k+1}{2}\rfloor}Q_k^m\left(\frac{\left<a,x\right>}{\left<x,x\right>}\right).\]
\end{proof}
We note that, up to scaling and rotation of the sphere, $F_k^m(x)$ as defined above is the unique degree $k$ polynomial with all three of these properties. These polynomials have played an important role in understanding spherical $t$-distance sets as well as $t$-designs (\cite{Delsarte1977},\cite{Suda2011}). Many of these results come from considering a finite set of points on the sphere and a basis of the harmonic polynomials on the sphere with fixed degree. We then create vectors for each point in our set by evaluating each basis polynomial at those points. Taking inner products we may then use Theorem \ref{gegprop} to show that the resulting values must be equal to a Gegenbauer polynomial evaluated at the inner product of our two points. It should not be surprising that these polynomials were of interest before \cite{Delsarte1977}, in fact 
35 years earlier, Sch\"{o}nberg characterized positive definite functions on the sphere using these same polynomials. We will now consider this theorem and investigate the consequences as they pertain to association schemes.
\section{Sch\"{o}nberg's theorem}
Let $m$ be a fixed positive integer as before and let $X\subset S^{m-1}$ be a finite set of unit vectors. Let $G_X$ denote the Gram matrix of $X$, then $G_X$ is positive semi-definite ($G_X\succeq 0$). A function $f:[-1,1]\rightarrow\mathbb{R}$ is \textit{positive definite} if, for every such finite subset $X$, $f\circ(G_X)\succeq 0$.
\begin{thm}[Sch\"{o}nberg \cite{Schoenberg1942}]\label{schoenthm}
	Fix $m\in\mathbb{Z}^+$. A function $f:[-1,1]\rightarrow\mathbb{R}$ is positive definite on $S^{m-1}$ if and only if $f(t) = \sum_{i} c_iQ_k^m(t)$ for non-negative constants $c_i$.\qed
\end{thm}
In particular, this means that $Q_k^m(t)$ is a positive definite function for any choice of $m$ and $k$. This leads theorem \ref{schoen-as}:
\schAS
\begin{proof}
Since $\BMA =\text{span}\left\{E_0,\dots,E_d\right\}$ is closed under entrywise products, we must have $Q_k^{m_i}\circ\left(\frac{\vert X\vert}{m_i}E_i\right)\in \BMA$ and we may write $Q_k^{m_i}\circ\left(\frac{\vert X\vert}{m_i}E_i\right) = \sum_j \theta_{kj} E_j$ where each $\theta_{kj}$ is the eigenvalue of $Q_k^{m_i}\circ\left(\frac{\vert X\vert}{m_i}E_i\right)$ in the eigenspace $V_j$. Now recall there exists an algebra isomorphism $\phi^*:\BMA\rightarrow\bbL$ via $\phi^*\left(E_i\right)=\frac{1}{\vert X\vert}L_i^*$ extended linearly. Further $\phi^*$ has the property that $\phi^*(E_i\circ E_j) = \phi^*(E_i)\phi^*(E_j)$. Applying $\phi^*$ to both sides of the first equation results in the second. Finally, since $E_i$ is an idempotent matrix with constant main diagonal entries given by $\frac{1}{\vert X\vert}Q_{0i} = \frac{m_i}{\vert X\vert}$, we know that $\frac{\vert X\vert}{m_i}E_i$ is the Gram matrix of a set of points in $S^{m_i-1}$. Therefore, Theorem~\ref{schoenthm} tells us that $Q_k^{m_i}\circ\left(\frac{\vert X\vert}{m_i}E_i\right)$ must be positive semidefinite.
\end{proof}
\subsection{Proof of Theorem \ref{degreebound}}
Recall from \ref{kreinidentities} that for each $0\leq i\leq d$, $q^j_{j0} = \delta_{i,j}$. Let $\vec{c}_k$ be the first column of the matrix $Q_{k}^{m_i}\left(\frac{1}{m_i}L_i^*\right)$ and using our recurrence relation \eqref{recurrence} we have,
\begin{equation}\label{crec}
\vec{c}_k = \frac{(2k+m_i-4)\frac{1}{m_i}L_i^*\vec{c}_{k-1} - (k-1)\vec{c}_{k-2}}{k+m-3} \qquad k\geq 2.
\end{equation}
From equation $\eqref{ELGeg}$, we see that the entries of $\vert X\vert \vec{c}_k$ are the eigenvalues of $Q_{k}^{m_i}\left(\frac{\vert X\vert}{m_i}E_i\right)$. Since $Q_0^m\left(\frac{\vert X\vert}{m_i}E_i\right) = J$ and $Q_1^m\left(\frac{\vert X\vert}{m_i}E_i\right) = \frac{\vert X\vert}{m_i}E_i$, we know that
\begin{equation*}
\vec{c}_0 = \vec{e}_0\qquad \vec{c}_1 = \frac{1}{m_i}\vec{e}_i
\end{equation*}
where $\vec{e}_i$ is the $i^\text{th}$ standard basis vector. While these vectors have a convenient relation with the matrices in equation \eqref{ELGeg}, we find it useful to instead consider the vectors $\vec{b}_k = \sqrt{\Delta_m}\vec{c}_k$ where $\sqrt{\Delta_m}$ is the diagonal matrix with $i^\text{th}$ diagonal entry $\sqrt{m_i}$. This transformation turns our recurrence relation into:
\begin{equation}\label{brec}
\vec{b}_k = \frac{(2k+m_i-4)M\vec{b}_{k-1} - (k-1)\vec{b}_{k-2}}{k+m-3} \qquad k\geq 2,
\end{equation}
where $M = \frac{1}{m_i}\sqrt{\Delta_m}L_i^*\left(\sqrt{\Delta_m}\right)^{-1}$. We also have the initial conditions
\begin{equation*}
\vec{b}_0 = \vec{e}_0\qquad \vec{b}_1 = \frac{1}{\sqrt{m_i}}\vec{e}_i.
\end{equation*}
Before moving on, we prove a lemma concerning $M$ which is the main motivation for making this transformation.
\begin{lem}\label{Mmat}
	Let $M = \frac{1}{m_i}\sqrt{\Delta_m}L_i^*\left(\sqrt{\Delta_m}\right)^{-1}$. Then the vectors $\left\{\vec{p}_0,\vec{p}_1,\dots,\vec{p}_d\right\}$ with
	\[p_j = \left[\begin{array}{c}
	P_{0j}\\
	\sqrt{m_1}P_{1j}\\
	\vdots\\
	\sqrt{m_d}P_{dj}\\
	\end{array}\right]\]
	serve as an orthogonal set of eigenvectors for $M$ with corresponding eigenvalues $\frac{Q_{0i}}{m_i},\frac{Q_{1i}}{m_i},\dots,\frac{Q_{di}}{m_i}$.
\end{lem}
\begin{proof}
First recall that the eigenvectors of $L_i^*$ are the columns of $P$ with eigenvalues $Q_{0i},Q_{1i},\dots,Q_{di}$. Since we are conjugating by an invertible matrix, we have the eigenvectors of $M$ are the columns of $\sqrt{\Delta_m}P$ with the eigenvalues scaled by our constant $\frac{1}{m_i}$. Now, using our orthogonality relations, we have that $\Delta_m P = Q^T \Delta_v$ and $PQ = \vert X\vert I$. Therefore
\[\left(\sqrt{\Delta_m}P\right)^T\left(\sqrt{\Delta_m}P\right) = P^T\Delta_mP = P^TQ^T\Delta_v = \vert X\vert\Delta_v.\]
\end{proof}
Now that we have an orthogonal set of eigenvectors, we define a final set of vectors for which the recurrence relation becomes linear. Let 
\begin{equation}\label{tkdef}\mu_k = \frac{k-1}{k+m-3};\qquad \vec{y}_k = \left[\begin{array}{c}
\vec{b}_{k}\\\hdashline[2pt/2pt]
\vec{b}_{k-1}
\end{array}\right];\qquad T_k = \left[\begin{array}{>{\centering\arraybackslash}p{3cm};{2pt/2pt}>{\centering\arraybackslash}p{3cm}}
 $(1+\mu_k)M$ & $-\mu_k I$\\\hdashline[2pt/2pt]
$I$ & $0$
\end{array}\right].\end{equation}
Then we have
\begin{equation}\label{yrec}
	\vec{y}_k = T_k\vec{y}_{k-1}.
\end{equation}
While $T_k$ depends on $k$, we identify some common properties among the eigenvalues and eigenvectors of $T_k$ for all values of $k$. We will prove a few lemmas which work towards Theorem \ref{degreebound}.
\begin{lem}\label{Tkeig}
	Let $(\lambda,\vec{v})$ be an eigenpair of $M$. Define $\left\{\eta_\lambda^+,\eta_\lambda^-\right\}$ be the two roots of the quadratic equation $\eta^2-(1+\mu_k)\lambda\eta +\mu_k=0$. If $\eta_\lambda^+\neq\eta_\lambda^-$ then both
	$\left(\eta_\lambda^+,\left[\begin{array}{c}
	\eta_\lambda^+\vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right]\right)$ and $ \left(\eta_\lambda^+,\left[\begin{array}{c}
	\eta_\lambda^+\vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right]\right)$
	are eigenpairs of $T_k$. If instead $\eta_\lambda^+=\eta_\lambda^-$ then $\left(\eta_\lambda^+,\left[\begin{array}{c}
	\eta_\lambda^+\vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right]\right)$ is still an eigenpair with a generalized eigenvector $\left(\eta_\lambda^+,\left[\begin{array}{c}
		\vec{v}\\\hdashline[2pt/2pt]
		\vec{0}
	\end{array}\right]\right)$
\end{lem}
\begin{proof}
	Let $M \vec{v} = \lambda\vec{v}$ and let $\eta$ be given so that $\eta^2-(1+\mu_k)\lambda\eta +\mu_k=0$. Then,
	\[T_k\left[\begin{array}{c}
	\eta \vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right] = \left[\begin{array}{c}
	(1+\mu_k)M(\eta \vec{v}) - \mu \vec{v}\\\hdashline[2pt/2pt]
	\eta \vec{v}
	\end{array}\right] = \left[\begin{array}{c}
	\left((1+\mu_k)\lambda\eta - \mu_k\right) \vec{v}\\\hdashline[2pt/2pt]
	\eta \vec{v}
	\end{array}\right] =  \left[\begin{array}{c}
	\eta^2 \vec{v}\\\hdashline[2pt/2pt]
	\eta \vec{v}
	\end{array}\right].\]
	Since our only requirement on $\eta$ was that it satisfied our quadratic equation, both roots will result in $T_k\left[\begin{array}{c}
	\eta \vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right] =  \eta\left[\begin{array}{c}
	\eta \vec{v}\\\hdashline[2pt/2pt]
	 \vec{v}
	\end{array}\right].$ If we have two distinct roots $(\eta_\lambda^+\neq \eta_\lambda^-)$ then the vectors $\left[\begin{array}{c}
	\eta_\lambda^+ \vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right]$ and $\left[\begin{array}{c}
		\eta_\lambda^- \vec{v}\\\hdashline[2pt/2pt]
		\vec{v}
	\end{array}\right]$ are linearly independent and we have two distinct eigenvectors. If instead we find that $\eta^2-(1+\mu_k)\lambda\eta +\mu_k=0$ has a double root, then $(1+\mu_k)\lambda^2-4\mu_k=0$ and thus $\eta = \frac{(1+\mu_k)\lambda}{2}$. In this case, note that
	\[T_k\left[\begin{array}{c}
	\vec{v}\\\hdashline[2pt/2pt]
	\vec{0}
	\end{array}\right] = \left[\begin{array}{c}
	(1+\mu_k)M(\vec{v})\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right]= \left[\begin{array}{c}
	(1+\mu_k)\lambda\vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right] = \left[\begin{array}{c}
	\eta \vec{v}\\\hdashline[2pt/2pt]
	\vec{0}
	\end{array}\right]+\left[\begin{array}{c}
	\eta \vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right].\]
	Thus $(T_k-\eta I)\left[\begin{array}{c}
	\vec{v}\\\hdashline[2pt/2pt]
	\vec{0}
	\end{array}\right] = \left[\begin{array}{c}
	\eta \vec{v}\\\hdashline[2pt/2pt]
	\vec{v}
	\end{array}\right]$ giving us that $\left[\begin{array}{c}
	\vec{v}\\\hdashline[2pt/2pt]
	\vec{0}
	\end{array}\right]$ is a generalized eigenvector. Note that the restriction $(1+\mu_k)\lambda^2-4\mu_k=0$ is quite strong and requires that $k = \frac{\lambda^2(m-2)}{2\left(\pm\sqrt{1-\lambda^2}-(1-\lambda^2)\right)}+m-1$. Since $k$ is a positive integer, this means that $\frac{\lambda^2(m-2)}{2\left(\pm\sqrt{1-\lambda^2}-(1-\lambda^2)\right)}\in\bbZ$. Thus we can check ahead of time if this case will ever arise.
\end{proof}
Lemma \ref{Tkeig} gives a complete description of the eigenspaces of our transition matrix $T_k$ since we know $M$ is diagonalizable with orthogonal eigenvectors and thus the eigenvectors of $T_k$ may be split into pairs where vectors from distinct pairs are orthogonal to each other. Our next lemma examines one pair and bounds the action of $T_k$ on that subspace.
\begin{lem}\label{singleroot}
	Let $\left(\lambda,\vec{p}\right)$ be an eigenpair of $M$ and assume that $(1+\mu_k)^2\lambda^2-4\mu_k>0$ and thus $x^2-(1+\mu_k)\lambda x + \mu_k$ has two distinct real roots. Let $\vec{v} = \left[\begin{array}{c}
	a\vec{p}\\\hdashline[2pt/2pt]
	b\vec{p}
	\end{array}\right]$ for $b>0$. Then $\lambda a\geq0$ and $\vert\frac{a}{b}\vert\leq\vert\lambda\vert\leq 1$ together imply $\vert\vert T_k \vec{v}\vert\vert_2\leq \vert \lambda\vert\cdot\vert\vert \vec{v}\vert\vert_2$.
\end{lem}
\begin{proof}
	We begin by noting that since our operator $T_k$ is linear, we may assume without loss of generality that $b=1$ and $\vert\vert\vec{p}\vert\vert_2 = 1$. Next, as in Lemma \ref{Tkeig}, define $\eta^+$ and $\eta^-$ to be the two roots of the polynomial $x^2-(1+\mu_k)\lambda x + \mu_k$, giving corresponding eigenvectors 
	\[\vec{v}^+ = \left[\begin{array}{c}
	\eta^+ \vec{p}\\\hdashline[2pt/2pt]
	\vec{p}
	\end{array}\right];\qquad \vec{v}^-=\left[\begin{array}{c}
	\eta^- \vec{p}\\\hdashline[2pt/2pt]
	\vec{p}
	\end{array}\right].\]
	Note as well that this implies $\eta^++\eta^-=(1+\mu_k)\lambda$ and $\eta^+\eta^- = \mu_k$. Using these definitions for $\vec{v}^+$ and $\vec{v}^-$, we find $\vec{v} = \left(\frac{\eta^--a}{\eta^--\eta^+}\right)\vec{v}^+ + \left(\frac{a-\eta^+}{\eta^--\eta^+}\right)\vec{v}^-$. We may then calculate $T_k v$ explicitly giving
	\[T_k \vec{v} = \left(\frac{\eta^--a}{\eta^--\eta^+}\right)\eta^+\vec{v}^+ + \left(\frac{a-\eta^+}{\eta^--\eta^+}\right)\eta^-\vec{v}^- = \left[\begin{array}{c}
	\beta \vec{p}\\\hdashline[2pt/2pt]
	a\vec{p}
	\end{array}\right]\]
	where $\beta = \left(\frac{\eta^--a}{\eta^--\eta^+}\right)\left(\eta^+\right)^2 + \left(\frac{a-\eta^+}{\eta^--\eta^+}\right)\left(\eta^-\right)^2$. Thus,
	\[\begin{aligned}\vert\vert T_k \vec{v}\vert\vert^2_2 &= \beta^2+a^2\\
	&=\frac{\left(\eta^--a\right)^2\left(\eta^+\right)^4 + \left(\eta^+\eta^-\right)^2\left(\eta^--a\right)\left(a-\eta^+\right) +\left(a-\eta^+\right)^2\left(\eta^-\right)^4}{\left(\eta^--\eta^+\right)^2}+a^2\\
	&=\left(\left(\eta^-+\eta^+\right)^2+1\right)a^2 - 2\eta^-\eta^+\left(\eta^-+\eta^+\right)a + \left(\eta^-\eta^+\right)^2\\
	&=\left((1+\mu_k)^2\lambda^2+1\right)a^2 - 2\mu_k(1+\mu_k)\lambda a + \mu_k^2\\
	&=\left[(\lambda a-1)^2\mu_k^2 + 2\lambda a(\lambda a-1)\mu+a^2-\lambda^2\right]+\lambda^2(a^2+1)\end{aligned}\]
	The roots of the bracketed polynomial are
	\[\mu_k = \frac{\lambda a \pm \sqrt{a^2(\lambda^2-1)+\lambda^2}}{1-\lambda a}.\]
	Note however that we assumed $(1+\mu_k)^2\lambda^2-4\mu_k>0$ and thus we know
	\[\mu_k\leq \frac{2(1-\sqrt{1-\lambda^2})-\lambda^2 }{\lambda^2}\qquad\text{ or }\qquad\mu_k\geq \frac{2(1+\sqrt{1-\lambda^2})-\lambda^2 }{\lambda^2}.\]
	The condition on the right is impossible since $0<\mu_k<1$ and therefore all that remains to show is the right most inequality of 
	\[\frac{\lambda a -\sqrt{\lambda^2 a^2-a^2+\lambda^2}}{1-\lambda a}\leq 0\leq\mu_k\leq \frac{2(1-\sqrt{1-\lambda^2})-\lambda^2 }{\lambda^2}\leq\frac{\lambda a + \sqrt{a^2(\lambda^2-1)+\lambda^2}}{1-\lambda a}\]
	whenever $\vert a\vert<\vert \lambda\vert$. To show this, note that
	\[\frac{2(1-\sqrt{1-\lambda^2})-\lambda^2}{\lambda^2}\leq\lambda^2\leq\sqrt{a^2(\lambda^2-1)+\lambda^2}\leq\frac{\lambda a+\sqrt{a^2(\lambda^2-1)+\lambda^2}}{1-\lambda a}\]
	when $\vert \lambda\vert\leq 1$. Since $\mu_k$ is bounded between the roots of $(\lambda a-1)^2\mu_k^2 + 2\lambda a(\lambda a-1)\mu_k+a^2-\lambda^2$, we know that $(\lambda a-1)^2\mu_k^2 + 2\lambda a(\lambda a-1)\mu_k+a^2-\lambda^2\leq 0$ giving
	\[\vert\vert T_k \vec{v}\vert\vert^2_2\leq \lambda^2(a^2+1) = \lambda^2\vert\vert \vec{v}\vert\vert^2_2\]
	as desired.
\end{proof}
\begin{lem}\label{doubleroot}
	Let $\left(\lambda,\vec{p}\right)$ be an eigenpair of $M$ and assume that $(1+\mu_k)^2\lambda^2-4\mu_k=0$ and thus $x^2-(1+\mu_k)\lambda x + \mu_k$ has a double root. Let $\vec{v} = \left[\begin{array}{c}
	a\vec{p}\\\hdashline[2pt/2pt]
	b\vec{p}
	\end{array}\right]$ for $b>0$. Then $\lambda a\geq0$ and $\vert\frac{a}{b}\vert\leq\vert\lambda\vert\leq 1$ together imply $\vert\vert T_k \vec{v}\vert\vert_2\leq \vert \lambda\vert\cdot\vert\vert \vec{v}\vert\vert_2$.
\end{lem}
\begin{proof}
	As with our previous lemma, we assume without loss of generality that $b=1$ and $\vert\vert\vec{p}\vert\vert_2 = 1$. Next, as in Lemma \ref{Tkeig}, define $\eta = \frac{(1+\mu_k)\lambda}{2}$ to be the double root of $x^2-(1+\mu_k)\lambda x + \mu_k$, giving eigenvector and generalized eigenvector
	\[\vec{w} = \left[\begin{array}{c}
	\eta \vec{p}\\\hdashline[2pt/2pt]
	\vec{p}
	\end{array}\right];\qquad \vec{w}^*=\left[\begin{array}{c}
	\vec{p}\\\hdashline[2pt/2pt]
	\vec{0}
	\end{array}\right].\]
	Then $\vec{v} = \vec{w} + \left(a-\eta\right)\vec{w}^*$ and thus $T_k\vec{v} =  a\vec{w} + \eta\left(a-\eta\right)\vec{w}^* = \left[\begin{array}{c}
	(2a-\eta)\eta\vec{p}\\\hdashline[2pt/2pt]
	a\vec{p}
	\end{array}\right].$
	Therefore
	\[\begin{aligned}\vert\vert T_k\vec{v}\vert\vert_2^2 &= (2a-\eta)^2\eta^2 + a^2\\
	&=(4a^2-2a(1+\mu_k)\lambda + \mu_k)\mu_k+a^2\\
	&=(1+\mu_k)^2\lambda^2a^2-2a(1+\mu_k)\lambda\mu_k+\mu_k^2+a^2\\
	&=\left[(\lambda a-1)^2\mu^2 + 2\lambda a(\lambda a-1)\mu+ a^2-\lambda^2\right] + \lambda^2(1+a^2)
	\end{aligned}\]
	giving $\vert \vert T_k \vec{v}\vert\vert_2^2\leq \vert\lambda\vert^2\vert\vert\vec{v}\vert\vert_2^2$ as before.
\end{proof}
\begin{lem}\label{complexroot}
	Let $\left(\lambda,\vec{p}\right)$ be an eigenpair of $M$ and assume that $(1+\mu_k)^2\lambda^2-4\mu_k<0$ and thus $x^2-(1+\mu_k)\lambda x + \mu_k$ has no real roots. Let $\vec{v} = \left[\begin{array}{c}
	a\vec{p}\\\hdashline[2pt/2pt]
	b\vec{p}
	\end{array}\right]$ for $b>0$. $\vert\vert T_k \vec{v}\vert\vert_2=\sqrt{\mu_k}\cdot\vert\vert \vec{v}\vert\vert_2$.
\end{lem}
\begin{proof}
	Let $\eta^+$ and $\eta^-$ be the two roots of $x^2-(1+\mu_k)\lambda x + \mu_k$. Since $\eta^+$ and $\eta^-$ are not real, $\vert\eta^+\vert = \vert\eta^-\vert = \sqrt{\eta^+\eta^-} = \sqrt{\mu_k}$. Therefore every vector in this subspace is rotated and scaled down by a factor of $\sqrt{\mu_k}$.
\end{proof}
\degbnd
\begin{proof}
	Theorem \ref{ycequiv} gives us that the condition given in equation \eqref{ELGeg} for fixed $k>0$ is equivalent to the condition that $y_k\geq 0$ for the $y_k$ given by the recurrence relation equation \eqref{yrec}. Since the initial condition $y_1$ is given by 
	\[y_1 = \left[\begin{array}{c}
	\vec{b}_{1}\\\hdashline[2pt/2pt]
	\vec{b}_{0}
	\end{array}\right] = \left[\begin{array}{c}
	\frac{1}{\sqrt{m_i}}\vec{e}_i\\\hdashline[2pt/2pt]
	\vec{e}_1
	\end{array}\right].\]
	Recall that the eigenspaces of $T_k$ may be paired so as to partition $\mathbb{R}^{2d}$ into
	
	\[B_j = \text{span}\left(\left[\begin{array}{c}
	\sqrt{\Delta_m}\vec{p}_j\\\hdashline[2pt/2pt]
	\vec{0}
	\end{array}\right],\left[\begin{array}{c}
	\vec{0}\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{p}_j
	\end{array}\right]\right)\] where $\vec{p}_j$ is the $j^\text{th}$ column of $P$. First note that 
	\[\text{proj}_{B_j}(\vec{y}_1) = \frac{1}{v_j\vert X\vert}\left[\begin{array}{c}
	P_{i,j}\sqrt{\Delta_m}\vec{p}_i\\\hdashline[2pt/2pt]
	P_{0,j}\sqrt{\Delta_m}\vec{p}_i
	\end{array}\right] =\frac{1}{\vert X\vert}\left[\begin{array}{c}
	\frac{P_{i,j}}{v_j}\sqrt{\Delta_m}\vec{p}_i\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{p}_i
	\end{array}\right] = \frac{1}{\vert X\vert}\left[\begin{array}{c}
	\lambda_j\sqrt{\Delta_m}\vec{p}_i\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{p}_i
	\end{array}\right] \]
	giving that
	\[\vert\vert\text{proj}_{B_j}(y_1)\vert\vert_2^2 = \frac{v_j\vert X\vert}{\vert X\vert^2}(1+\lambda_j^2) =\frac{v_j}{\vert X\vert}(1+\lambda_j^2). \]
	This allows us to split $y_1$ into
	\[y_1 = \frac{1}{\vert X\vert}\left[\begin{array}{c}
	\sqrt{\Delta_m}\vec{1}\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{1}
	\end{array}\right] + \frac{1}{\vert X\vert}\sum_{j=1}^d\left[\begin{array}{c}
	\lambda_j\sqrt{\Delta_m}\vec{p}_i\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{p}_i
	\end{array}\right]\]
	giving us
	\[\vec{y}_{k^*+1} = \left(\prod_{k=2}^{k^*+1}T_k\right)\vec{y}_1 = \frac{1}{\vert X\vert}\left[\begin{array}{c}
	\sqrt{\Delta_m}\vec{1}\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{1}
	\end{array}\right] + \frac{1}{\vert X\vert}\sum_{j=1}^d\left(\prod_{k=2}^{k^*+1}T_k\right)\left[\begin{array}{c}
	\lambda_j\sqrt{\Delta_m}\vec{p}_i\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{p}_i
	\end{array}\right].\]
	Note that the smallest element of $\frac{1}{\vert X\vert}\left[\begin{array}{c}
	\sqrt{\Delta_m}\vec{1}\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{1}
	\end{array}\right]$ is exactly $\frac{1}{\vert X\vert}$ and thus in order for $\vec{y}_{k^*+1}$ to have a negative element, we must have 
	\[\left\vert\left\vert \frac{1}{\vert X\vert}\sum_{j=1}^d\left(\prod_{k=2}^{k^*+1}T_k\right)\left[\begin{array}{c}
	\lambda_j\sqrt{\Delta_m}\vec{p}_i\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{p}_i
	\end{array}\right]\right\vert\right\vert^2_2>\frac{1}{\vert X\vert^2}.\]
	Simplifying the left hand side we have
	\[\begin{aligned}\left\vert\left\vert \frac{1}{\vert X\vert}\sum_{j=1}^d\left(\prod_{k=2}^{k^*+1}T_k\right)\left[\begin{array}{c}
	\lambda_j\sqrt{\Delta_m}\vec{p}_i\\\hdashline[2pt/2pt]
	\sqrt{\Delta_m}\vec{p}_i
	\end{array}\right]\right\vert\right\vert^2_2 &= \sum_{j=1}^d\frac{v_j}{\vert X\vert}\left\vert\left\vert \left(\prod_{k=2}^{k^*+1}T_k\right)\left[\begin{array}{c}
	\lambda_j\hat{\vec{b}}_j\\\hdashline[2pt/2pt]
	\hat{\vec{b}}_j
	\end{array}\right]\right\vert\right\vert^2_2\\
	&\leq \frac{1}{\vert X\vert}\sum_{j=1}^d v_j\lambda_j^{2k^*}(1+\lambda_j^2)
	\end{aligned}\]
	where $\hat{\vec{b}}_j = \frac{\vec{p}_j}{\vert \vert \vec{p}_j\vert\vert_2}$. Therefore, if we have
	\[\sum_{j=1}^d v_j\lambda_j^{2k^*}(1+\lambda_j^2)\leq\frac{1}{\vert X\vert}\]
	then we guarantee $y_{k^*+1}$ has no negative entry. Finally since the bottom half of $y_{k^*+1}$ equals the top half of $y_{k^*}$, we may assume that any negative entry of $y_{k^*}$ appears in the bottom half, implying that $y_{k^*-1}$ has a negative entry in the top half. Thus checking  the first $k^*-1$ conditions is sufficient.
\end{proof}
\begin{cor}
	Let $(X,\mathcal{R})$ be an association scheme with first and second eigenmatrices $P$ and $Q$. Let $0\leq i\leq d$ be given and define $\lambda_j:=\frac{Q_{j,i}}{m_j}$. Further assume $1=\lambda_0>\vert\lambda^*\vert\geq\vert\lambda_j\vert$ for $0<j<d$. Then $k^*$ as defined in Theorem \ref{degbnd} follows
	\[k^*\leq\left\lceil\frac{\ln\left[(1+(\lambda^*)^2)\vert X\vert(\vert X\vert-1)\right]}{-2\ln(\lambda^*)}\right\rceil.\]
\end{cor}
\begin{proof}
	\[\sum_{j=1}^d v_j\lambda_j^{2x}(1+\lambda_j^2)\leq(\lambda^*)^{2x}(1+(\lambda^*)^2)\sum_{j=1}^d v_j =(\lambda^*)^{2x}(1+(\lambda^*)^2)(\vert X\vert -1). \]
	Solving 
	\[(\lambda^*)^{2x}(1+(\lambda^*)^2)(\vert X\vert -1)\leq\frac{1}{\vert X\vert}\]
	for $x$ gives
	\[x\geq\frac{\ln\left[(1+(\lambda^*)^2)\vert X\vert(\vert X\vert-1)\right]}{-2\ln(\lambda^*)}.\]
	Thus $\left\lceil\frac{\ln\left[(1+(\lambda^*)^2)\vert X\vert(\vert X\vert-1)\right]}{-2\ln(\lambda^*)}\right\rceil$ is the smallest integer for which our relaxed inequality holds, giving $k^*\leq \left\lceil\frac{\ln\left[(1+(\lambda^*)^2)\vert X\vert(\vert X\vert-1)\right]}{-2\ln(\lambda^*)}\right\rceil$.
\end{proof}

\section{Cometric case}
In this section, we restrict to the case of cometric association schemes and explicitly compute the first few bounds given in Theorem \ref{schoen-as}. We will also apply these bounds specifically in the imprimitive case. Let $(X,\mathcal{R})$ be given and assume there exists a $Q$-polynomial ordering $E_0,E_1,\dots, E_d$ with natural ordering $A_0,A_1,\dots,A_d$. Let $\left\{1,b^*_1,\dots,b^*_{d-1};c^*_1,c_2^*\dots,c^*_{d-1}\right\}$ be the Krein array of $(X,\mathcal{R})$ and define $m:=\text{rank}(E_1)$. As before, let $\theta_{k,i}$ denote the eigenvalue of $Q_k^m\circ\left(\frac{\vert X\vert}{m}E_1\right)$ on the eigenspace $V_i$ for $k>0$ and $0\leq i\leq d$, allowing ourselves to omit the comma when the meaning is clear. From our initial two Gegenbauer polynomials, we know
\[\theta_{00} = 1;\qquad \theta_{11} =\frac{1}{m}.\]
Using equation \eqref{gegprop} and our Krein array, we have
 \begin{equation}
 \theta_{ki}= \frac{(2k+m-4)(b_{i}^*\theta_{k-1,i+1} +a_i^*\theta_{k-1,i}+c_{i}^*\theta_{k-1,i-1}) - (k-1)m\theta_{k-2,i}}{m(k+m-4)}.
 \end{equation}
 Below we give the eigenvalues for degrees $2\leq k\leq 6$ omitting $\theta_{ki}$ for $i>k$ as these eigenvalues must be $0$ due the $Q$-polynomial property. For convenience we define the parameters $a_i^*,b_i^*,$ and $c_i^*$ for $i\in\mathbb{Z}^+$ noting that $c_{j-1}^* = b_j^*=a_j^*=0$ for $j>d$.
\subsection*{Degree 2 constraint}
\[\theta_{20}  = 0;\qquad\theta_{21}  = \frac{a_1^*}{m(m-1)};\qquad\theta_{22} = \frac{c_2^*}{m(m-1)}.\]
Since the condition that $\theta_{2i}\geq 0$ is implied by Krein conditions for each $0\leq i\leq d$, we consider these constraints vacuous.
\subsection*{Degree 3 constraint}
\[\begin{aligned}
F_3 &= \frac{(m+2)\frac{\vert X\vert}{m}E_1\circ F_{2} - (2)F_{1}}{m}\\
&=\frac{\frac{m+2}{m}\vert X\vert E_1\circ \left(\frac{a_1^* E_1 + c_2^*E_2}{m(m-1)}\right) - \left(\frac{2}{m}E_1\right)}{m}\\
&=\frac{(m+2)\left(a_1^*(mE_0 + a_1^*E_1+c_2^*E_2)+ c_2^*(b_1^*E_1 + a_2^*E_2+c_3^*E_3)\right) - \left(2mE_1\right)}{m^3(m-1)}\\
&=\frac{(m+2)}{m^3(m-1)}\left[(a_1^*m)E_0 + \left((a_1^*)^2 + b_1^*c_2^*-2m\right)E_1 + (a_1^*+a_2^*)c_2^*E_2 + c_2^*c_3^*E_3\right]
\end{aligned}\]
\[\begin{aligned}
\theta_{30} &= \frac{\frac{m+2}{m(m-1)}(a_1^*) - 0}{m(m-1)} = \frac{m+2}{m^2(m-1)^2}.
\end{aligned}\]
\[\begin{aligned}
\theta_{31} &= \frac{m(a_1^*\frac{1}{m}) }{m(m-1)} = \frac{a_1^*}{m(m-1)}.
\end{aligned}\]
\[\begin{aligned}
\theta_{32} &= \frac{m(c_2^*\frac{1}{m})}{m(m-1)} = \frac{c_2^*}{m(m-1)}.
\end{aligned}\]
\[\begin{aligned}
\theta_{33} &= \frac{m(c_2^*\frac{1}{m})}{m(m-1)} = \frac{c_2^*}{m(m-1)}.
\end{aligned}\]



Here, the only constraint not implied by Krein conditions is the eigenvalue in the $V_1$ eigenspace. Therefore, we must have
\begin{equation}
	(a_1^*)^2 + b_1^*c_2^*\geq 2m
\end{equation}





